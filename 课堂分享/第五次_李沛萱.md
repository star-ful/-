# Fedavg算法的基础原理

## 简介Fedavg框架
FedAvg是一种分布式框架，允许多个用户同时训练一个机器学习模型。<br>
简单的流程叙述：<br>
中心节点初始化一个模型，把这个初始模型发给其他节点，<br>
其他节点在自己的地方做训练，训练结束后，把训练好的模型传到中心节点。<br>
然后中心节点做一个加权平均。然后中心节点再把模型发给其他节点，<br>
重复上述广播模型，上传模型操作。<br>
一直到中心节点认为准确率达到要求后，停止操作。<br>

可以看到，联邦学习的过程中，中心节点是没有在本地存储数据的，这样就保证了数据的隐私。<br>
并且中心节点在没有数据的情况下，也能达到训练所有其他节点数据的效果。<br>

## 模型训练过程中的基础知识

### 数据重构

数据集的分割：<br>

1.数据分布为IID：<br>
IID：独立同分布 指一组随机变量中每个变量的概率分布是相同的，且这些随机变量互相独立。<br>
分割过程:<br>
随机抽样: 数据通常随机分为训练集和测试集，确保每个集的样本分布相似。<br>
均匀性: 由于数据是 IID 的，因此每个子集都能代表整个数据集的特征。<br>

2.数据分布为NON-IID：<br>
Non-IID 意味着数据并非是互相独立的，或者不同的子集可能遵循不同的分布。<br>
这样在分割数据集的时候就不能简单地随机分割<br>
有时需要使用聚类算法或分层抽样方法，以确保不同的分布或特征在各个子集中得到合理的表示。<br>

### 神经网络的前向传播和反向传播

在理解模型训练时，我们需要了解神经网络的前后向传播。<br>
1.前向传播：<br>
将输入数据通过神经网络的每一层进行计算，最终得到模型的输出。<br>

2.计算损失：<br>
前向传播最终生成的结果是根据目前模型的权重设置来计算得到的。<br>
由于训练过程是循序渐进的，第一次传播使用的权重值甚至是随机生成的，所以一定会跟真实值r有偏差（即为E）。<br>

3.反向传播：<br>
更新权重，使误差减小<br>
如何量化wi对Etotal的影响？<br>
![](https://github.com/star-ful/ZS/blob/%E7%AC%AC%E4%BA%94%E6%AC%A1_%E6%9D%8E%E6%B2%9B%E8%90%B1/Pic/image/8c82428323da5125d7184a5325ab397f.png)

从输出层开始，计算损失函数对输出的梯度。（使用链式法则）计算每层的权重和梯度<br>
得到所有参数的梯度后这些梯度将用于更新模型的参数。<br>

4.梯度下降:<br>
w=w−η⋅∇L(θ)<br>
∇L(θ)即为E对wi求偏导<br>

总结：<br>
前向传播 负责计算模型的输出和损失。<br>
后向传播 负责计算损失对各个参数的梯度。<br>
梯度下降 则使用这些梯度来更新模型的参数，以优化模型性能。<br>

以此我们也可以引出SGD<br>

### SGD
在神经网络中每次随机选一个数据做梯度下降训练。<br>
w=w−η⋅∇L(θ)<br>

## FedAvg的伪代码分析

C：每轮训练选择client的比例，每一轮通信时只选择C*K个client；（K为client总数）<br>
E：每个client更新本地权重时，在本地数据集上训练E轮；
B：client更新权重时，每次梯度下降所使用的数据量；<br>

先来看客户端的
```
ClientUpdate(k,w):
    for each local epoch i from 1 to E do
        batches <- (data Pk split into batches of size B)
        //  数据集Pk 按照大小 B 分割成多个小批次
        for batch b in batches do 
        //对于batches的每个b做梯度下降
            w <- w-η∇L(w;b)
    return w to server 
    //整个数据做完E轮训练后得到新模型，返回给server
```
再来看server的
```
Server executes:
    initialize w0
    for each round t = 1,2...do 
    //做x轮循环 也可以写成while循环直到准确率达到要求
        St = (random set of max(CK,1)clients) 
        //C是选取的比例，K是客户端的数量
        //在众多客户端中随机选择C*K个做梯度下降，得到新的St
        for each client k∈St in parallel do
            wt+1 <- ClientUpdate(k,wt)
        wt+1 <- Σt=1->t=k nk/n *wt 
        //将客户端的模型收回，做加权平均
        //nk是参与者 𝑘拥有的本地训练样本的数量。
        //n是所有参与者（或所有客户端）的样本总数
        //nk/n即为对于
```
